{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import scholarly\n",
    "os.chdir('C:\\\\Users\\\\Cafral\\\\Desktop\\\\kaggle\\\\CORD-19-research-challenge\\\\data_v7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cafral\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique papers in method section :  9693  out of  188137  rows in dataframe\n",
      "No of unique papers in result section :  7820  out of  178851  rows in dataframe\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188137 entries, 0 to 188136\n",
      "Data columns (total 38 columns):\n",
      "Unnamed: 0                         188137 non-null int64\n",
      "paper_id                           188137 non-null object\n",
      "language                           188137 non-null object\n",
      "section                            188137 non-null object\n",
      "sentence                           188137 non-null object\n",
      "lemma                              188137 non-null object\n",
      "UMLS                               188137 non-null object\n",
      "GGP                                188137 non-null object\n",
      "SO                                 188137 non-null object\n",
      "TAXON                              188137 non-null object\n",
      "CHEBI                              188137 non-null object\n",
      "GO                                 188137 non-null object\n",
      "CL                                 188137 non-null object\n",
      "DNA                                188137 non-null object\n",
      "CELL_TYPE                          188137 non-null object\n",
      "CELL_LINE                          188137 non-null object\n",
      "RNA                                188137 non-null object\n",
      "PROTEIN                            188137 non-null object\n",
      "DISEASE                            188137 non-null object\n",
      "CHEMICAL                           188137 non-null object\n",
      "CANCER                             188137 non-null object\n",
      "ORGAN                              188137 non-null object\n",
      "TISSUE                             188137 non-null object\n",
      "ORGANISM                           188137 non-null object\n",
      "CELL                               188137 non-null object\n",
      "AMINO_ACID                         188137 non-null object\n",
      "GENE_OR_GENE_PRODUCT               188137 non-null object\n",
      "SIMPLE_CHEMICAL                    188137 non-null object\n",
      "ANATOMICAL_SYSTEM                  188137 non-null object\n",
      "IMMATERIAL_ANATOMICAL_ENTITY       188137 non-null object\n",
      "MULTI-TISSUE_STRUCTURE             188137 non-null object\n",
      "DEVELOPING_ANATOMICAL_STRUCTURE    188137 non-null object\n",
      "ORGANISM_SUBDIVISION               188137 non-null object\n",
      "CELLULAR_COMPONENT                 188137 non-null object\n",
      "PATHOLOGICAL_FORMATION             188137 non-null object\n",
      "ORGANISM_SUBSTANCE                 188137 non-null object\n",
      "sentenc_id                         1047 non-null object\n",
      "sentence_id                        187090 non-null object\n",
      "dtypes: int64(1), object(37)\n",
      "memory usage: 54.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_method = pd.read_csv('method_df.csv')\n",
    "df_result = pd.read_csv('result_df.csv')\n",
    "\n",
    "print(\"No of unique papers in method section : \", df_method['paper_id'].nunique(), \" out of \", \n",
    "      len(df_method), \" rows in dataframe\")\n",
    "print(\"No of unique papers in result section : \", df_result['paper_id'].nunique(), \" out of \", \n",
    "      len(df_result), \" rows in dataframe\")\n",
    "\n",
    "df_method.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting sentences which contain topic ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 312 sentences containing keywords/ngrams in Method section.\n",
      "There are 256 sentences containing keywords/ngrams in Result section.\n",
      "Total unique papers in Method section : 222\n",
      "Total unique papers in Result section : 100\n",
      "Total unique papers in combined section : 296\n"
     ]
    }
   ],
   "source": [
    "def find_ngrams(dataframe,columnToSearch,keywords):\n",
    "    df_w_ngrams = dataframe[dataframe[columnToSearch].str.contains('|'.join(keywords), case=False) == True]\n",
    "    return df_w_ngrams\n",
    "\n",
    "ngrams = [' humidity',' rain is',' rain was',' rain has',' damp weather', ' damp climate',\n",
    "          ' monsoon',' rainy',' water vapour',' rainfall']#'sweat','damp',\n",
    "\n",
    "#Extracting sentences which contain ngrams\n",
    "\n",
    "df_method_p = find_ngrams(df_method,'sentence',ngrams)\n",
    "\n",
    "df_result_p = find_ngrams(df_result,'sentence',ngrams)\n",
    "\n",
    "print(\"There are {} sentences containing keywords/ngrams in Method section.\".format(len(df_method_p)))\n",
    "print(\"There are {} sentences containing keywords/ngrams in Result section.\".format(len(df_result_p)))\n",
    "\n",
    "# Merging the method and result section sentences into single dataframe\n",
    "df_real = pd.concat([df_method_p, df_result_p])\n",
    "\n",
    "print(\"Total unique papers in Method section : {}\".format(df_method_p['paper_id'].nunique()))\n",
    "print(\"Total unique papers in Result section : {}\".format(df_result_p['paper_id'].nunique()))\n",
    "print(\"Total unique papers in combined section : {}\".format(df_real['paper_id'].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping all the sentences from papers that had topic ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique papers in combined section : 296\n"
     ]
    }
   ],
   "source": [
    "df_method_all_sentence = pd.merge(df_method[['paper_id','sentence']],df_method_p['paper_id'],on='paper_id',how='right')\n",
    "df_method_all_sentence.rename(columns={'sentence_x':'all_sentences','sentence_y':'ngram_sentence'},inplace=True)\n",
    "\n",
    "df_result_all_sentence = pd.merge(df_result[['paper_id','sentence']],df_result_p['paper_id'],on='paper_id',how='right')\n",
    "df_result_all_sentence.rename(columns={'sentence_x':'all_sentences','sentence_y':'ngram_sentence'},inplace=True)\n",
    "\n",
    "df_all_sentences = pd.concat([df_method_all_sentence, df_result_all_sentence])\n",
    "print(\"Total unique papers in combined section : {}\".format(df_all_sentences['paper_id'].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting methodolody,sample size,causal nature,sentences refering to coronavirus, fatality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(ngramDf,allSentdataFrame):\n",
    "    # extracting methodology\n",
    "    methods_list = ['regression','OLS','ordinary least squares','logistic regression' , 'neural network',\n",
    "                    'random forest','logistic function','time series','model','modelling','simulation',\n",
    "                    'forecast','forecasting']\n",
    "    methodology = find_ngrams(allSentdataFrame,'sentence',methods_list)\n",
    "\n",
    "    #extracting sample size\n",
    "    sample_size_list = ['population size','sample size','number of samples','number of observations',\n",
    "                        'number of subjects']\n",
    "    sample_size = find_ngrams(allSentdataFrame,'sentence',sample_size_list)\n",
    "\n",
    "    #extracting nature of correlation\n",
    "    causal_list =['statistically significant','statistical significance',\n",
    "                  'correlation','positively correlated','negatively correlated','correlated',\n",
    "                  'p value','p-value','chi square','chi-square','t statistic','standard error'\n",
    "                  'confidence interval','odds ratio','coefficient']\n",
    "\n",
    "    causality_type = find_ngrams(allSentdataFrame,'sentence',causal_list)\n",
    "\n",
    "    # extracting coronavirus related sentence #can someone check and update this list?\n",
    "    coronavirus_list = ['severe acute respiratory syndrome','sars-cov','sars-like',\n",
    "                        'middle east respiratory syndrome','mers-cov','mers-like',\n",
    "                        'covid-19','sars-cov-2','2019-ncov','sars-2',\n",
    "                        'sarscov-2','novel coronavirus','corona virus','coronaviruses',\n",
    "                        'sars','mers','covid19','covid 19']\n",
    "\n",
    "    coronavirus = find_ngrams(allSentdataFrame,'sentence',coronavirus_list)\n",
    "\n",
    "    # extracting outcome\n",
    "    disease_stage_list = ['lethal', 'morbid',\"death\", \"fatality\", \"mortality\",\"lethal\", \"lethality\", \"morbidity\"]\n",
    "\n",
    "    fatality = find_ngrams(allSentdataFrame,'sentence',disease_stage_list)\n",
    "\n",
    "    df_list = [methodology,sample_size,causality_type,coronavirus,fatality]\n",
    "    df_list_name = ['methodology','sample_size','causality_type','coronavirus','fatality']\n",
    "    i=0\n",
    "    for one_df in df_list:\n",
    "        one_df.rename(columns={'sentence':df_list_name[i]},inplace=True)\n",
    "        grouped_one_df = one_df.groupby(['paper_id'], as_index=False)[df_list_name[i]].sum()\n",
    "        ngramDf = pd.merge(ngramDf,grouped_one_df,on='paper_id',how='left')\n",
    "        i=i+1\n",
    "    return ngramDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real = extract_features(df_real,df_all_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 568 entries, 0 to 567\n",
      "Data columns (total 21 columns):\n",
      "paper_id          568 non-null object\n",
      "language          568 non-null object\n",
      "section           568 non-null object\n",
      "sentence          568 non-null object\n",
      "lemma             568 non-null object\n",
      "UMLS              568 non-null object\n",
      "sentence_id       567 non-null object\n",
      "publish_time      524 non-null object\n",
      "authors           521 non-null object\n",
      "url               523 non-null object\n",
      "methodology       363 non-null object\n",
      "sample_size       74 non-null object\n",
      "causality_type    298 non-null object\n",
      "coronavirus       144 non-null object\n",
      "fatality          108 non-null object\n",
      "title             524 non-null object\n",
      "abstract          515 non-null object\n",
      "publish_time      524 non-null object\n",
      "authors           521 non-null object\n",
      "url               523 non-null object\n",
      "TAXON             568 non-null object\n",
      "dtypes: object(21)\n",
      "memory usage: 97.6+ KB\n"
     ]
    }
   ],
   "source": [
    "metadata = pd.read_csv('clean_metadata.csv')\n",
    "metadata.rename(columns={'sha':'paper_id'}, inplace = True)\n",
    "metadata['paper_id'] = metadata['paper_id'].astype(\"str\")\n",
    "\n",
    "#Merging the given papers with their metadata\n",
    "df_real = df_real.merge(metadata[['paper_id', 'title', 'abstract', 'publish_time', 'authors',\n",
    "                                  'url']], on='paper_id', how='left') #'title_w_ngram','abstract_w_ngram'\n",
    "\n",
    "#Keeping only the fields which are relevant to us.\n",
    "df_real = df_real[['paper_id','language', 'section', 'sentence', 'lemma', 'UMLS', 'sentence_id', \n",
    "                   'publish_time', 'authors', 'url','methodology','sample_size','causality_type','coronavirus',\n",
    "                   'fatality','title','abstract','publish_time','authors',\n",
    "                   'url','TAXON']]#'title_w_ngram','abstract_w_ngram',\n",
    "df_real.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cafral\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:63: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped = df_real.groupby('paper_id')\n",
    "def keywordcounter(sentences, keywords_list):\n",
    "    '''\n",
    "    Input : List of sentences, List of keywords\n",
    "    Returns : Keywords present in sentences, Total count of all keywords present in Input\n",
    "    '''\n",
    "    keyword = {}\n",
    "    sent = \" \".join(sentences)\n",
    "    for pol in keywords_list:\n",
    "        counter = sent.lower().count(pol)\n",
    "        if (counter > 0):\n",
    "            keyword[pol] = counter\n",
    "    return list(keyword.keys()), sum(keyword.values())\n",
    "\n",
    "def aggregation(item,keyWordList,RiskFactor):\n",
    "    '''\n",
    "    Input : Dataframe of sentences of a paper\n",
    "    Return : Datframe in Standard Output format\n",
    "    '''\n",
    "    dfo = {}\n",
    "    \n",
    "    dfo['Risk Factor'] = RiskFactor\n",
    "    dfo['Title'] = item['title'].iloc[0]\n",
    "    dfo['Keyword/Ngram'], dfo['No of keyword occurence in Paper'] = keywordcounter(item['sentence'].tolist(),\n",
    "                                                                                 keyWordList)\n",
    "    dfo['paper_id'] = item['paper_id'].iloc[0]\n",
    "    \n",
    "    if (item['url'].iloc[0].isnull().any()==False):\n",
    "        dfo['URL'] = item['url'].iloc[0].tolist()\n",
    "    else:\n",
    "        dfo['URL']=''\n",
    "    #dfo['Sentences from Title']= item['title_w_ngram'].iloc[0]                        \n",
    "    #dfo['Sentences from Abstract']= item['abstract_w_ngram'].iloc[0]\n",
    "    dfo['Sentences from Method'] = item[item['section']=='methods']['sentence'].tolist()\n",
    "    dfo['Sentences from Result'] = item[item['section']=='results']['sentence'].tolist()\n",
    "    \n",
    "    if (item['authors'].iloc[0].isnull().any()==False):#(item['authors'].iloc[0].isnull()==False):\n",
    "        dfo['Authors'] = item['authors'].iloc[0].tolist()\n",
    "    else:\n",
    "         dfo['Authors'] = ''\n",
    "    # For papers which do not have title (not in metadata) we have to resolve exceptions\n",
    "    #try:\n",
    "    #    dfo['No of Citations'] = next(scholarly.search_pubs_query(item['title'].iloc[0])).citedby\n",
    "    #except:\n",
    "    dfo['No of Citations'] = 0\n",
    "        \n",
    "    dfo['Correlation'] = item['causality_type'].iloc[0]\n",
    "    dfo['Design Methodology'] = item['methodology'].iloc[0]\n",
    "    dfo['Sample Size'] = item['sample_size'].iloc[0]\n",
    "    dfo['Coronavirus'] = item['coronavirus'].iloc[0]\n",
    "    dfo['Fatality'] = item['fatality'].iloc[0]\n",
    "    dfo['TAXON'] =item['TAXON'].iloc[0]\n",
    "    \n",
    "    return dfo\n",
    "\n",
    "df_output = pd.DataFrame(columns=['Risk Factor', 'Title','Keyword/Ngram', 'No of keyword occurence in Paper',\n",
    "                                  'paper_id', 'URL',\n",
    "                                  'Sentences from Result', 'Sentences from Method',\n",
    "                                  'Authors','No of Citations', 'Correlation', \n",
    "                                  'Design Methodology', 'Sample Size',\n",
    "                                 'Coronavirus','Fatality','TAXON'])#Sentences from Title','Sentences from Abstract',\n",
    "for key, item in grouped:\n",
    "    df_output = pd.concat([df_output, pd.DataFrame([aggregation(item,ngrams,'Humidity')])])\n",
    "\n",
    "df_output = df_output.reset_index()\n",
    "df_output.to_excel('humidity_json.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
